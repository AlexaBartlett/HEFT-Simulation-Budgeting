{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import chaospy as cp\n",
    "import yaml\n",
    "import h5py\n",
    "import json\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.interpolate import make_smoothing_spline\n",
    "from glob import glob\n",
    "import GPy\n",
    "\n",
    "plt.rcParams['axes.labelsize']        = 12\n",
    "plt.rcParams['axes.titlesize']        = 12\n",
    "plt.rcParams['xtick.labelsize']       = 12\n",
    "plt.rcParams['ytick.labelsize']       = 12\n",
    "plt.rcParams['font.family']           = 'serif'\n",
    "plt.rcParams['font.size']             = 12\n",
    "\n",
    "def norm(x, x_mean=None, x_mult=None):\n",
    "\n",
    "    if x_mean is None:\n",
    "        x_mean = np.mean(x, axis=0)\n",
    "\n",
    "    if x_mult is None:\n",
    "        x_mult = 2 / (np.max(x, axis=0) - np.min(x, axis=0))\n",
    "\n",
    "    x_normed = (x - x_mean[np.newaxis, ...]) * x_mult[np.newaxis, ...]\n",
    "\n",
    "    return x_normed, x_mean, x_mult\n",
    "\n",
    "\n",
    "def unnorm(x_normed, x_mean, x_mult):\n",
    "\n",
    "    x = x_normed / x_mult[np.newaxis, ...] + x_mean[np.newaxis, ...]\n",
    "\n",
    "    return x\n",
    "\n",
    "class Emulator(object):\n",
    "\n",
    "    def __init__(self, n_pcs, k, smooth_spectra=True, window=11, savgol_order=3,\n",
    "                 surrogate_type='PCE'):\n",
    "        \n",
    "        self.n_pcs = n_pcs\n",
    "        self.smooth_spectra = smooth_spectra\n",
    "        self.window = window\n",
    "        self.savgol_order = savgol_order\n",
    "        self.nk = len(k)\n",
    "        self.k = k\n",
    "        self.surrogate_type = surrogate_type\n",
    "        \n",
    "\n",
    "    \n",
    "    def _get_pcs(self, evec_spec, spectra, npc):\n",
    "\n",
    "        pcs_spec = np.dot(spectra, evec_spec[:, :npc])\n",
    "\n",
    "        return pcs_spec\n",
    "\n",
    "\n",
    "    def train(self, x, y, npoly=[1], qtrunc=1):\n",
    "        \n",
    "        evec_spec = np.zeros((self.nk, self.nk))\n",
    "        \n",
    "        if not hasattr(npoly, '__iter__'):\n",
    "            npoly = [npoly] * x.shape[1]\n",
    "        elif len(npoly) == 1:\n",
    "            npoly *= x.shape[1]\n",
    "            \n",
    "        # computing PCs\n",
    "        if self.smooth_spectra:\n",
    "            ysmooth = savgol_filter(y, self.window,\n",
    "                                 self.savgol_order, axis=-1)\n",
    "        else:\n",
    "            ysmooth = y\n",
    "        \n",
    "        ycov = np.cov(ysmooth.T)\n",
    "        self.vars_spec, self.evec_spec = np.linalg.eig(ycov)\n",
    "        pcs_spec = np.dot(ysmooth, self.evec_spec[:, :self.n_pcs])\n",
    "\n",
    "        x_normed, param_mean, param_mult = norm(x)\n",
    "        self.param_mean = param_mean\n",
    "        self.param_mult = param_mult\n",
    "        self.param_ranges_scaled = np.vstack([np.min(x_normed, axis=0), np.max(x_normed, axis=0)])\n",
    "        self.param_ranges_scaled = self.param_ranges_scaled.T\n",
    "\n",
    "        if self.surrogate_type == 'PCE':\n",
    "\n",
    "            distribution = cp.J(*[cp.Uniform(self.param_ranges_scaled[i][0],\n",
    "                                             self.param_ranges_scaled[i][1]) for i in range(x_normed.shape[1])])\n",
    "\n",
    "            # PCE coefficient regression\n",
    "            pce = cp.orth_ttr(npoly, distribution,\n",
    "                                  cross_truncation=qtrunc, graded=True)\n",
    "            # print(pce.shape)\n",
    "            # print(pce)\n",
    "            self.surrogate = cp.fit_regression(pce, x_normed.T, np.real(pcs_spec))\n",
    "            \n",
    "        if self.surrogate_type == 'GP':\n",
    "            # Uncomment the below line to use isotropic RBF kernel\n",
    "            # kernel = GPy.kern.RBF(input_dim=x_normed.shape[1], variance=1., lengthscale=1.)\n",
    "\n",
    "            # Anisotropic RBF kernel\n",
    "            kernel = GPy.kern.RBF(input_dim=x_normed.shape[1], ARD=True, lengthscale=np.ones(x_normed.shape[1]).tolist())\n",
    "            self.surrogate = GPy.models.GPRegression(x_normed, np.real(pcs_spec), kernel=kernel)\n",
    "            self.surrogate.optimize()\n",
    "            print(self.surrogate.flattened_parameters) #printing length scales\n",
    "            # np.savetxt('emulator_errors/pmm_w0wamnu_noisy_ZCV_tier2/pmm_w0wamnu_noisy_ZCV_tier2_v4_GP_length_params.txt', self.surrogate.flattened_parameters)\n",
    "            \n",
    "\n",
    "    def predict(self, x):\n",
    "        xnorm, _, _ = norm(x, self.param_mean, self.param_mult)\n",
    "        if self.surrogate_type == 'PCE':\n",
    "            ypred = self.surrogate(*(xnorm.T))\n",
    "            ypred = np.dot(self.evec_spec[:,:self.n_pcs], ypred).T\n",
    "\n",
    "        else:\n",
    "            ypred, _ = self.surrogate.predict(xnorm)\n",
    "            ypred = np.dot(self.evec_spec[:,:self.n_pcs], ypred.T).T\n",
    "\n",
    "        \n",
    "        return ypred\n",
    "\n",
    "# try 3 and 4 for order\n",
    "def compute_error_stats(P, F, F_c, P_test, F_test, F_c_test, k, npc=10,\n",
    "                        order=2, use_z=False, logmnu=False,\n",
    "                        return_all_err=False, surrogate_type='PCE'):\n",
    "    \"\"\"\n",
    "    P : array training parameters with shape (N_sims, N_pars). Right now order is expected to be As, ns, H0, w0, wa, ombh2, omch2, nu_mass_ev, time (e.g. redshift)\n",
    "    F: array of simulation measurements we wish to emulate with shape (N_sims, nk).\n",
    "    F_c : array of LPT predictions at the same cosmologies/redshifts as the sim measurements with shape (N_sims, nk). We emulate the ratio of F/F_c.\n",
    "    P_test: array test parameters with shape (N_test, N_pars).\n",
    "    F_test: array of test measurements we wish to measure the emulator error against with shape (N_test, nk).\n",
    "    F_c_test: array of LPT predictions at the same cosmologies/redshifts as F_test with shape (N_test, nk).\n",
    "    k: array of wavenumbers of shape (nk)\n",
    "    npc: Number of principle components to decompose F/Fc into.\n",
    "    order: Order of the PCE (if using PCE as surrogate type)\n",
    "    use_z: Whether to use redshift or scale factor as the time variable. Only relevant if time variable being passed in P/P_test is redshift.\n",
    "    logmnu: If true, use logmnu instead of mnu as parameter. Expects this to be the second to last parameter in P/Ptest.\n",
    "    \n",
    "    returns:\n",
    "    stderr: The 50th, 68th, 95th and 99th percentile error averaged across k for each point in test parameter space.\n",
    "    kerr: The 50th, 68th, 95th and 99th percentile error as a function of k over the whole test parameter space.\n",
    "    emu: Emulator object.\n",
    "    \"\"\"\n",
    "\n",
    "    Ptrain = np.copy(P)\n",
    "    F_r = F / F_c\n",
    "    F_r_test = F_test/ F_c_test\n",
    "\n",
    "    # These for loops use a spline to smooth over wiggles in F_r, significantly reducing emulator error\n",
    "    #   for lower-k, especially for tier 1. If you don't wish to smooth, comment-out the for-loops.\n",
    "    \n",
    "    lam = 0.005\n",
    "    idx_min = 190\n",
    "    idx_max = 525\n",
    "\n",
    "    for i in range(F_r.shape[0]):\n",
    "        spl = make_smoothing_spline(k[idx_min:idx_max], (F_r)[i,idx_min:idx_max], lam=lam, w=10*np.array(1-np.exp(-((k[idx_min:idx_max]-0.4)/(0.1))**2)))\n",
    "        F_r[i,idx_min:idx_max] = spl(k[idx_min:idx_max])\n",
    "\n",
    "    for i in range(F_r_test.shape[0]):\n",
    "        spl = make_smoothing_spline(k[idx_min:idx_max], (F_r_test)[i,idx_min:idx_max], lam=lam, w=10*np.array(1-np.exp(-((k[idx_min:idx_max]-0.4)/(0.1))**2)))\n",
    "        F_r_test[i,idx_min:idx_max] = spl(k[idx_min:idx_max])\n",
    "\n",
    "\n",
    "    if logmnu:\n",
    "        Ptrain[:,-2] = 10**Ptrain[:,-2]\n",
    "        P_test[:,-2] = 10**P_test[:,-2]\n",
    "        \n",
    "    Pmean = np.mean(Ptrain, axis=0)\n",
    "    Pstd = np.std(Ptrain, axis=0)\n",
    "    Ptrain_norm = (Ptrain - Pmean) / Pstd\n",
    "    Ptest_norm = (P_test - Pmean) / Pstd  \n",
    "    \n",
    "    Ftrain =  np.log10(F_r)\n",
    "    Ftrain[Ftrain!=Ftrain] = 0\n",
    "    \n",
    "    k = np.logspace(-3, 0, 100)\n",
    "    emu = Emulator(npc, k, surrogate_type=surrogate_type)\n",
    "    emu.train(Ptrain_norm, Ftrain, npoly=order)\n",
    "    \n",
    "    pmin = np.min(Ptrain_norm, axis=0)\n",
    "    pmax = np.max(Ptrain_norm, axis=0)\n",
    "    delta_p = (pmax - pmin)\n",
    "    frac_design = 1\n",
    "    pmin = pmin + delta_p * (1-frac_design) / 2 \n",
    "    pmax = pmax - delta_p * (1-frac_design) / 2\n",
    "\n",
    "    idx = np.all((Ptest_norm >= pmin[np.newaxis,:]) & (Ptest_norm <= pmax[np.newaxis,:]), axis=1)\n",
    "    Fpred = emu.predict(Ptest_norm[idx])\n",
    "    err = np.abs((10**Fpred - F_r_test[idx])/ F_r_test[idx])\n",
    "    x = P_test[idx]\n",
    "    stderr = np.percentile(err, [50, 66, 95, 99], axis=1)\n",
    "    kerr = np.percentile(err, [50, 66, 95, 99], axis=0)\n",
    "       \n",
    "    return stderr, kerr, emu # Fpred, Ptest_norm, F_r_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_max = 539 #k=1\n",
    "# idx_max = 497 #k=0.7\n",
    "# idx_max = 479 #k=0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dir = 'training_data/' # where all training data lives\n",
    "data_dir = 'example_data/' # where data to be used to train this emulator lives\n",
    "hmcode_training_data_file=train_dir+data_dir+'pk_hmcode.npy'\n",
    "lpt_training_data_file=train_dir+data_dir+'pk_1L.npy' # N cosmologies, 30 redshifts, 700 ks\n",
    "# lpt_training_data_file=train_dir+data_dir+'pk_lin.npy' # 200 cosmologies, 30 redshifts, 700 ks\n",
    "training_cosmo_file=train_dir+data_dir+'params.txt' # N cosmologies, 8 cosmological parameters\n",
    "zs = np.load('./aemulus_data/zs.npy') # 30 redshifts\n",
    "\n",
    "sigma_8 = np.loadtxt(train_dir+data_dir+'sigma_8.txt') # sigma_8(z) for all runs\n",
    "k = np.loadtxt(train_dir+data_dir+'k_out.txt') # 700 ks\n",
    "\n",
    "F = np.load(hmcode_training_data_file)\n",
    "F_c = np.load(lpt_training_data_file)\n",
    "P = np.genfromtxt(training_cosmo_file)\n",
    "\n",
    "# Uncomment below to train on just the last X cosmologies in a dataset. Set X=0 to use all. Use idx_max to truncate k-range.\n",
    "X = 400\n",
    "F = F[-X:, :, :idx_max]\n",
    "F_c = F_c[-X:, :, :idx_max]\n",
    "P = P[-X:,:]\n",
    "sigma_8 = sigma_8[-X:]\n",
    "\n",
    "print(P.shape)\n",
    "\n",
    "# If you want to, for example, train on wCDM, you should remove the parameter column corresponding to w_a \n",
    "#   by uncommenting the below lines\n",
    "# print(P[0])\n",
    "# P = np.delete(P, [3], axis=1)\n",
    "# print(P.shape)\n",
    "\n",
    "P = np.repeat(P, F.shape[1], axis=0) # repeat N cosmologies 30 times: (N*30, 7)\n",
    "P = np.hstack((P, sigma_8.flatten().reshape(len(sigma_8.flatten()),1))) # adding sigma_8 as final column\n",
    "\n",
    "F = F.reshape(-1, F.shape[-1]) # Shape: 6000, 700\n",
    "F_c = F_c.reshape(-1, F_c.shape[-1]) # Shape: 6000, 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = np.loadtxt(train_dir+data_dir+'k_out.txt') # 700 ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dir = 'testing_data/' # where all testing data lives\n",
    "hmcode_test_data_file=test_dir+data_dir+'pk_hmcode.npy'\n",
    "lpt_test_data_file=test_dir+data_dir+'pk_1L.npy'\n",
    "test_cosmo_file=test_dir+data_dir+'params.txt'\n",
    "\n",
    "sigma_8_test = np.loadtxt(test_dir+data_dir+'sigma_8.txt') #sigma_8(z) for all runs\n",
    "F_test = np.load(hmcode_test_data_file)\n",
    "F_c_test = np.load(lpt_test_data_file)\n",
    "P_test = np.genfromtxt(test_cosmo_file)\n",
    "\n",
    "X = 100\n",
    "F_test = F_test[-X:, :, :idx_max]\n",
    "F_c_test = F_c_test[-X:, :, :idx_max]\n",
    "P_test = P_test[-X:,:]\n",
    "sigma_8_test = sigma_8_test[-X:]\n",
    "\n",
    "P_test = np.repeat(P_test, F_test.shape[1], axis=0)\n",
    "P_test = np.hstack((P_test, sigma_8_test.flatten().reshape(len(sigma_8_test.flatten()),1)))\n",
    "\n",
    "F_test = F_test.reshape(-1, F_test.shape[-1])\n",
    "F_c_test = F_c_test.reshape(-1, F_test.shape[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stderr_full, kerr_full, emu = compute_error_stats(P, F, F_c, P_test, F_test, F_c_test, k[:idx_max], order=3)\n",
    "savedir = 'emulator_errors'\n",
    "# create emulator_errors directory before running\n",
    "# np.savetxt('emulator_errors/k.txt', k)\n",
    "np.savetxt(savedir+'/'+'example_stderr.txt', stderr_full)\n",
    "np.savetxt(savedir+'/'+'example_kerr.txt', kerr_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.semilogx(k[:idx_max], kerr_full.T)\n",
    "plt.xlim(0.05,1)\n",
    "plt.ylim(0,0.08)\n",
    "plt.legend(['50th percentile', '68th percentile', '95th percentile', '99th percentile'])\n",
    "plt.xlabel(r'$k\\, [h^{-1}\\, {\\rm Mpc}]$')\n",
    "plt.ylabel('Fractional emulator error')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
